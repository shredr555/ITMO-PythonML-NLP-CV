{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLYpSPDlW9LC"
      },
      "source": [
        "## Скачиваем необходимое\n",
        "\n",
        "Сначала нужно средствами NLTK загрузить WordNet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhTQ6EFXZ5R9",
        "outputId": "48e9320b-7e45-41de-bcac-28945b4c9c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQ4rEytvwLZ"
      },
      "source": [
        "## Готовим данные к работе\n",
        "\n",
        "Затем импортируем данные из подготовленного текстового файла. Файл содержит набор пар слов (только имён существительных), для которых известны экспертные оценки сходства.\n",
        "\n",
        "Строим ассоциативный массив \"пара слов -- оценка близости\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sTFACx3dAk8"
      },
      "outputs": [],
      "source": [
        "with open(\"Task_4_sample_3.csv\", encoding=\"utf-8\") as rf:\n",
        "  triples = [line.strip().split(\",\") for line in rf.readlines()]\n",
        "  triples.pop(0)\n",
        "  score_map = {tuple(triple[:2]): float(triple[2]) for triple in triples}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gbpnaX5gD6I"
      },
      "source": [
        "Отметим, что из исходного набора данных мы взяли только экспертные оценки сходства (similarity) и только для существительных. Исходный набор данных доступен по [ссылке](http://alfonseca.org/pubs/ws353simrel.tar.gz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96B0OtKrvtaG"
      },
      "source": [
        "Посмотрим на примеры оценок. \n",
        "\n",
        "У слов может быть по несколько значений, которые различаются в WordNet. Здесь -- ради примера -- мы будем \"жадно\" выбирать первое попавшееся, но далее будем работать с ними иначе.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iXamIiZgf-O"
      },
      "outputs": [],
      "source": [
        "# for w1, w2 in list(score_map):\n",
        "  \n",
        "#   print(\"\\nWords: %s-%s\\nGround truth score: %.2f\" % (w1, w2, score_map[(w1, w2)]))\n",
        "  \n",
        "#   ss1 = wn.synset(w1 + \".n.01\")\n",
        "#   ss2 = wn.synset(w2 + \".n.01\")\n",
        "\n",
        "#   print(\"\\nPath: %.3f\" % ss1.path_similarity(ss2), end=\" \")\n",
        "#   print(\"\\nwup: %.3f\" % ss1.wup_similarity(ss2), end=\" \")\n",
        "#   print(\"\\nshortest_path: %.3f\" % ss1.shortest_path_distance(ss2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHM7tCb0vqNp"
      },
      "source": [
        "По ссылке представлена выборка из датасета WordSim353. Ваша задача, используя тезаурус WordNet, вычислить оценки близости для всех элементов соответствующих синсетов представленных пар слов. В качестве близости двух слов использовать наибольшую близость среди соответствующих элементов рассматриваемых синсетов. В качестве меры близости использовать: близость на основе пути (path_similarity), меру Leacock-Chodorow (lch_similarity) и меру Wu-Palmer (wup_similarity). Вычислить коэффициент ранговой корреляции Спирмена для каждой меры близости, используя известную экспертную оценку (колонка Score в выборке)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe7Nuglqgnd3"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "list_pairs = list(score_map)\n",
        "path_list, lch_list, wup_list, true_list = [], [], [], []\n",
        "\n",
        "# для всех пар\n",
        "for w1, w2 in list_pairs:\n",
        "\n",
        "  try:\n",
        "    all_w1 = wn.synsets(w1, pos=\"n\")\n",
        "    all_w2 = wn.synsets(w2, pos=\"n\")\n",
        "\n",
        "    # добавляем интересующие нас метрики и экспертные оценки\n",
        "    path = max([item1.path_similarity(item2) \\\n",
        "                for item1, item2 in product(all_w1, all_w2)])\n",
        "    path_list.append(path)\n",
        "\n",
        "    lch = max([item1.lch_similarity(item2) \\\n",
        "                for item1, item2 in product(all_w1, all_w2)])\n",
        "    lch_list.append(lch)\n",
        "\n",
        "    wup = max([item1.wup_similarity(item2) \\\n",
        "                for item1, item2 in product(all_w1, all_w2)])\n",
        "    wup_list.append(wup)\n",
        "    \n",
        "    true_list.append(score_map[(w1, w2)])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(w1, w2, \"error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAjuTLB0fD-I"
      },
      "source": [
        "## Вычисляем ранговую корреляцию Спирмена"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXnCdw8wxcVd",
        "outputId": "150d7b3f-abce-4a04-8829-5c1ac3ac5903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path Spearman R: 0.680\n",
            "lch Spearman R: 0.680\n",
            "wup  Spearman R: 0.693\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "coef, p = spearmanr(path_list, true_list)\n",
        "print(\"path Spearman R: %.3f\" % coef)\n",
        "\n",
        "coef, p = spearmanr(lch_list, true_list)\n",
        "print(\"lch Spearman R: %.3f\" % coef)\n",
        "\n",
        "coef, p = spearmanr(wup_list, true_list)\n",
        "print(\"wup  Spearman R: %.3f\" % coef)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "При помощи метода hyponyms() найдите количество гипонимов для синсета student.n.01, а также при помощи метода name() найдите значение первого в гипонима из списка.\n",
        "\n",
        "Введите количество гипонимов для синсета student.n.01.\n",
        "\n",
        "Первый элемент списка гипонимов синсета student.n.01."
      ],
      "metadata": {
        "id": "gVOUYtOnwMl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = len(wn.synset('student.n.01').hyponyms())\n",
        "\n",
        "print(count)\n",
        "[str(lemma.name()) for lemma in wn.synset('student.n.01').hyponyms()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPRi31QZAfNx",
        "outputId": "8aa11481-a2f6-477c-cb24-ca09cf0ec171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['art_student.n.01',\n",
              " 'auditor.n.02',\n",
              " 'catechumen.n.01',\n",
              " 'collegian.n.01',\n",
              " 'crammer.n.01',\n",
              " 'etonian.n.01',\n",
              " 'ivy_leaguer.n.01',\n",
              " 'law_student.n.01',\n",
              " 'major.n.03',\n",
              " 'medical_student.n.01',\n",
              " 'nonreader.n.01',\n",
              " 'overachiever.n.01',\n",
              " 'passer.n.03',\n",
              " 'scholar.n.03',\n",
              " 'seminarian.n.01',\n",
              " 'sixth-former.n.01',\n",
              " 'skipper.n.01',\n",
              " 'underachiever.n.01',\n",
              " 'withdrawer.n.05',\n",
              " 'wykehamist.n.01']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}